import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from transformers import SwinForImageClassification, DeiTForImageClassification
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, f1_score
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Step 1: Load and Preprocess CIFAR-100 dataset
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)

test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)
test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)
# Step 2: Load Swin Transformer and DeiT Models
swin_model = SwinForImageClassification.from_pretrained('microsoft/swin-base-patch4-window7-224').to(device)
deit_model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224').to(device)

# Fine-tune the last few layers of the Swin model
for param in swin_model.parameters():
    param.requires_grad = False
for param in swin_model.swin.encoder.layers[-1].parameters():  # Fine-tune the last layer
    param.requires_grad = True

# Fine-tune the final layer of DeiT
for param in deit_model.parameters():
    param.requires_grad = False
for param in deit_model.deit.encoder.layer[-1].parameters():
    param.requires_grad = True
# Step 3: Train Swin and DeiT Models Separately
criterion = nn.CrossEntropyLoss()

def train_model(model, train_loader, criterion, optimizer, num_epochs=5):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(images).logits  # Extract logits for training
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

# Train the Swin model
optimizer_swin = optim.Adam(swin_model.parameters(), lr=0.0001)
print("Training Swin Transformer...")
train_model(swin_model, train_loader, criterion, optimizer_swin, num_epochs=2)

# Train the DeiT model
optimizer_deit = optim.Adam(deit_model.parameters(), lr=0.0001)
print("Training DeiT...")
train_model(deit_model, train_loader, criterion, optimizer_deit, num_epochs=2)
# Step 4: Feature Extraction
def extract_features(model, dataloader):
    features_list = []
    labels_list = []

    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        for images, labels in dataloader:
            images = images.to(device)
            features = model(images).logits  # Extract logits
            features_list.append(features)
            labels_list.append(labels)

    features = torch.cat(features_list, dim=0)
    labels = torch.cat(labels_list, dim=0)
    return features, labels

print("Extracting features from Swin Transformer...")
swin_features, train_labels = extract_features(swin_model, train_loader)

print("Extracting features from DeiT...")
deit_features, _ = extract_features(deit_model, train_loader)
# Step 5: Concatenate Features
combined_features = torch.cat((swin_features, deit_features), dim=1)  # Concatenate feature vectors from both models

# Step 6: Train Hybrid Classifier on Combined Features
class HybridClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(HybridClassifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.ReLU(),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.fc(x)

# Initialize the hybrid classifier
num_classes = 100
input_size = combined_features.shape[1]
classifier = HybridClassifier(input_size=input_size, num_classes=num_classes).to(device)

# Loss and optimizer for the hybrid classifier
optimizer_classifier = optim.Adam(classifier.parameters(), lr=0.0001)

def train_classifier(classifier, features, labels, criterion, optimizer, num_epochs=5):
    classifier.train()
    dataset = torch.utils.data.TensorDataset(features, labels)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (inputs, targets) in enumerate(loader):
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()

            outputs = classifier(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(loader):.4f}")

# Train the hybrid classifier
print("Training hybrid classifier...")
train_classifier(classifier, combined_features, train_labels, criterion, optimizer_classifier, num_epochs=5)
import torch.nn.functional as F

# Step 7: Evaluate the Hybrid Model
def evaluate_classifier(classifier, swin_model, deit_model, test_loader):
    classifier.eval()
    swin_model.eval()
    deit_model.eval()

    test_loss = 0
    correct = 0
    all_targets = []
    all_predictions = []

    batch_losses = []
    batch_accuracies = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            swin_output = swin_model(data)
            deit_output = deit_model(data)

            swin_logits = swin_output.logits if hasattr(swin_output, 'logits') else swin_output
            deit_logits = deit_output.logits if hasattr(deit_output, 'logits') else deit_output

            combined_output = (swin_logits + deit_logits) / 2
            loss = F.cross_entropy(combined_output, target)
            test_loss += loss.item()

            pred = combined_output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            all_predictions.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

            batch_losses.append(loss.item())
            batch_accuracy = pred.eq(target).sum().item() / len(target)
            batch_accuracies.append(batch_accuracy)

    avg_test_loss = test_loss / len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    test_precision = precision_score(all_targets, all_predictions, average="weighted")
    test_f1 = f1_score(all_targets, all_predictions, average="weighted")

    print(f'Average test loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%')
    print(f'Test Precision: {test_precision:.4f}, Test F1 Score: {test_f1:.4f}')

    # Plotting the metrics
    plt.figure(figsize=(15, 10))

    # Plot test loss
    plt.subplot(2, 2, 1)
    plt.plot(batch_losses, label='Test Loss')
    plt.title('Test Loss')
    plt.xlabel('Batch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot accuracy
    plt.subplot(2, 2, 2)
    plt.plot(batch_accuracies, label='Test Accuracy')
    plt.title('Test Accuracy')
    plt.xlabel('Batch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot precision
    plt.subplot(2, 2, 3)
    plt.plot([test_precision] * len(batch_losses), label='Test Precision')
    plt.title('Test Precision')
    plt.xlabel('Batch')
    plt.ylabel('Precision')
    plt.legend()

    # Plot F1 Score
    plt.subplot(2, 2, 4)
    plt.plot([test_f1] * len(batch_losses), label='Test F1 Score')
    plt.title('Test F1 Score')
    plt.xlabel('Batch')
    plt.ylabel('F1 Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

# Example usage
print("Evaluating hybrid model...")
evaluate_classifier(classifier, swin_model, deit_model, test_loader)
