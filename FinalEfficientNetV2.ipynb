!pip install tensorflow
!pip install keras-tuner
!pip install tensorflow-datasets
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import tensorflow_datasets as tfds
from tensorflow.keras.applications import efficientnet_v2
from tensorflow.keras.layers import Rescaling, RandomTranslation
from keras_tuner import RandomSearch
# Load CIFAR-100 dataset
def load_cifar100():
    (train_data, test_data), info = tfds.load('cifar100',
                                               split=['train', 'test'],
                                               as_supervised=True,
                                               with_info=True)
    return train_data, test_data, info
# Define image size to match EfficientNetV2 input (224x224)
IMG_SIZE = 224
BATCH_SIZE = 32  # Adjust batch size
AUTOTUNE = tf.data.AUTOTUNE

# ImageNet mean and std for normalization
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

# Preprocessing function with enhanced augmentation
def preprocess_image(image, label, is_training=True):
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])

    if is_training:
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_brightness(image, max_delta=0.1)
        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
        image = tf.image.random_hue(image, max_delta=0.1)
        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)

    image = Rescaling(1.0/255)(image)
    image = (image - IMAGENET_MEAN) / IMAGENET_STD
    return image, label

#  Create data pipeline
def create_data_pipeline(dataset, is_training=True):
    if is_training:
        dataset = dataset.shuffle(10000)
    dataset = dataset.map(lambda x, y: preprocess_image(x, y, is_training),
                          num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset
# Visualize example images with augmentation
def visualize_augmented_images(dataset, num_images=5):
    for image, label in dataset.take(num_images):
        plt.figure(figsize=(15, 15))
        for i in range(num_images):
            ax = plt.subplot(1, num_images, i + 1)
            plt.imshow(image[i].numpy())
            plt.title(f"Label: {label[i].numpy()}")
            plt.axis("off")
        plt.show()
# Load CIFAR-100 data
train_data, test_data, info = load_cifar100()
# Split the train_data into 80% training and 20% validation
train_size = int(0.8 * info.splits['train'].num_examples)

train_data_split = train_data.take(train_size)
val_data_split = train_data.skip(train_size)

# Create training, validation, and testing pipelines
train_dataset = create_data_pipeline(train_data_split, is_training=True)
val_dataset = create_data_pipeline(val_data_split, is_training=False)
test_dataset = create_data_pipeline(test_data, is_training=False)

# Visualize augmented training images
visualize_augmented_images(train_dataset)
import time
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, f1_score
from tensorflow.keras.callbacks import Callback

# Load EfficientNetV2 with pretrained weights
base_model = efficientnet_v2.EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
base_model.trainable = True  # Unfreeze the base model

# Fine-tune half the layers
fine_tune_at = len(base_model.layers) // 2
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Add a custom head for CIFAR-100 classification
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(100, activation='softmax')
])

# Compile the model without label smoothing
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=['accuracy'])

# Implement early stopping and model checkpointing
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', patience=5, restore_best_weights=True
)

model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'best_model.keras',  # Filepath corrected to end with .keras
    monitor='val_accuracy',
    save_best_only=True,
    mode='max'
)

# Train the model
history = model.fit(train_dataset,
                    epochs=30,  # Increased number of epochs
                    validation_data=val_dataset,
                    callbacks=[early_stopping, model_checkpoint])
import time
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, f1_score
from tensorflow.keras.callbacks import Callback
import tensorflow as tf
from tensorflow.keras.applications import efficientnet_v2
from tensorflow.keras.layers import Rescaling, RandomTranslation
from tensorflow.keras import metrics

# Define callbacks
class TimeHistory(Callback):
    def __init__(self):
        super().__init__()
        self.times = []

    def on_epoch_begin(self, epoch, logs=None):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, epoch, logs=None):
        self.times.append(time.time() - self.epoch_time_start)

class MetricsHistory(Callback):
    def __init__(self, val_data):
        super().__init__()
        self.val_data = val_data
        self.val_precision = []
        self.val_f1 = []

    def on_epoch_end(self, epoch, logs=None):
        val_true = []
        val_pred = []
        for x, y in self.val_data:
            val_true.extend(y.numpy())
            val_pred.extend(model.predict(x).argmax(axis=1))
        precision = precision_score(val_true, val_pred, average="weighted")
        f1 = f1_score(val_true, val_pred, average="weighted")
        self.val_precision.append(precision)
        self.val_f1.append(f1)

# Instantiate callbacks
time_callback = TimeHistory()
metrics_callback = MetricsHistory(val_dataset)

# Train the model with callbacks
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=val_dataset,
    callbacks=[time_callback, metrics_callback]
)

# Function to plot metrics
def plot_training_history(history, times, metrics_callback):
    plt.figure(figsize=(15, 10))

    # Plot accuracy
    plt.subplot(2, 3, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend(loc='upper left')

    # Plot loss
    plt.subplot(2, 3, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend(loc='upper left')

    # Plot precision
    plt.subplot(2, 3, 3)
    plt.plot(metrics_callback.val_precision, label='Validation Precision')
    plt.title('Model Precision')
    plt.xlabel('Epoch')
    plt.ylabel('Precision')
    plt.legend(loc='upper left')

    # Plot F1 score
    plt.subplot(2, 3, 4)
    plt.plot(metrics_callback.val_f1, label='Validation F1 Score')
    plt.title('Model F1 Score')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend(loc='upper left')

    # Plot training time per epoch
    plt.subplot(2, 3, 5)
    plt.plot(times, label='Training Time (s)')
    plt.title('Training Time per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Time (seconds)')
    plt.legend(loc='upper left')

    plt.tight_layout()
    plt.show()

# Visualize the training process
plot_training_history(history, time_callback.times, metrics_callback)

# Print the overall training time
print("Training time per epoch:", time_callback.times)
